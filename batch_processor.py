import asyncio
import sys
from typing import Optional
import pandas as pd
import os
import glob
import json
import tkinter as tk
from tkinter import scrolledtext, messagebox, ttk, filedialog
import threading
import datetime
import shutil

import re
import cmd_arg
import config
import db
import requests
from base.base_crawler import AbstractCrawler
from media_platform.bilibili import BilibiliCrawler
from media_platform.douyin import DouYinCrawler
from media_platform.kuaishou import KuaishouCrawler
from media_platform.tieba import TieBaCrawler
from media_platform.weibo import WeiboCrawler
from media_platform.xhs import XiaoHongShuCrawler
from media_platform.zhihu import ZhihuCrawler


import logging

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class CrawlerFactory:
    CRAWLERS = {
        "xhs": XiaoHongShuCrawler,
        "dy": DouYinCrawler,
        "ks": KuaishouCrawler,
        "bili": BilibiliCrawler,
        "wb": WeiboCrawler,
        "tieba": TieBaCrawler,
        "zhihu": ZhihuCrawler
    }

    @staticmethod
    def create_crawler(platform: str) -> AbstractCrawler:
        crawler_class = CrawlerFactory.CRAWLERS.get(platform)
        if not crawler_class:
            raise ValueError("Invalid Media Platform Currently only supported xhs or dy or ks or bili ...")
        return crawler_class()


def extract_links_from_text(text):
    """ä»æ–‡æœ¬ä¸­æå–é“¾æ¥ä¿¡æ¯"""
    lines = text.strip().split('\n')
    links_data = []
    
    for line in lines:
        if not line.strip():
            continue
            
        # æå–ç”¨æˆ·å
        username_match = re.search(r'(\S+?)å‘å¸ƒäº†', line)
        if not username_match:
            username_match = re.search(r'ã€([^ã€‘]+)', line)
        username = username_match.group(1) if username_match else "æœªçŸ¥ç”¨æˆ·"
        
        # æå–é“¾æ¥
        urls = re.findall(r'http[s]?://[^\s]+', line)
        
        for url in urls:
            link_data = {
                'username': username,
                'url': url,
                'platform': detect_platform(url),
                'content': line.strip()
            }
            links_data.append(link_data)
    
    return links_data

def detect_platform(url):
    """æ£€æµ‹é“¾æ¥å¹³å°"""
    if 'xiaohongshu.com' in url or 'xhslink.com' in url:
        return 'å°çº¢ä¹¦'
    elif 'douyin.com' in url:
        return 'æŠ–éŸ³'
    else:
        raise Exception(f"Unsupported platform for URL: {url}")


def save_to_excel(filename='social_media_stats.xlsx'):
    """ä¿å­˜ç»“æœåˆ°Excelæ–‡ä»¶ï¼Œä»JSONæ–‡ä»¶è¯»å–æ•°æ®"""
    
    # è¯»å–æŠ–éŸ³JSONæ•°æ®
    douyin_data = []
    douyin_json_path = "./data/douyin/json/"
    if os.path.exists(douyin_json_path):
        json_files = glob.glob(os.path.join(douyin_json_path, "*.json"))
        for json_file in json_files:
            try:
                with open(json_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    if isinstance(data, list):
                        for item in data:
                            douyin_data.append({
                                'username': item.get('nickname', 'æœªçŸ¥ç”¨æˆ·'),
                                'platform': 'æŠ–éŸ³',
                                'url': item.get('aweme_url', ''),
                                'likes': item.get('liked_count', '0'),
                                'comments': item.get('comment_count', '0'),
                                'collects': item.get('collected_count', '0'),
                                'shares': item.get('share_count', '0'),
                                'content': item.get('title', '')
                            })
            except Exception as e:
                logger.error(f"Failed to read {json_file}: {e}")
    
    # è¯»å–å°çº¢ä¹¦JSONæ•°æ®
    xhs_data = []
    xhs_json_path = "./data/xhs/json/"
    if os.path.exists(xhs_json_path):
        json_files = glob.glob(os.path.join(xhs_json_path, "*.json"))
        for json_file in json_files:
            try:
                with open(json_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    if isinstance(data, list):
                        for item in data:
                            xhs_data.append({
                                'username': item.get('nickname', 'æœªçŸ¥ç”¨æˆ·'),
                                'platform': 'å°çº¢ä¹¦',
                                'url': item.get('note_url', ''),
                                'likes': item.get('liked_count', '0'),
                                'comments': item.get('comment_count', '0'),
                                'collects': item.get('collected_count', '0'),
                                'shares': item.get('share_count', '0'),
                                'content': item.get('title', '')
                            })
            except Exception as e:
                logger.error(f"Failed to read {json_file}: {e}")
    
    # åˆå¹¶æ‰€æœ‰æ•°æ®
    all_data = douyin_data + xhs_data
    
    if not all_data:
        logger.warning("No data found in JSON files")
        return
    
    df = pd.DataFrame(all_data)
    
    # é‡æ–°æ’åˆ—åˆ—çš„é¡ºåº
    columns_order = [
        'username', 'platform', 'url', 
        'likes', 'comments', 'collects', 'shares', 'content'
    ]
    df = df[columns_order]
    
    # é‡å‘½ååˆ—
    df.columns = [
        'ç”¨æˆ·å', 'å¹³å°', 'é“¾æ¥', 
        'ç‚¹èµæ•°', 'è¯„è®ºæ•°', 'æ”¶è—æ•°', 'åˆ†äº«æ•°', 'åŸå§‹å†…å®¹'
    ]
    
    # ä¿å­˜åˆ°Excel
    filepath = f"{filename}"
    df.to_excel(filepath, index=False, engine='openpyxl')
    
    logger.info(f"Results saved to {filepath}")
    logger.info(f"Total records: {len(df)}")
    
    # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
    print(f"\n=== çˆ¬å–å®Œæˆ ===")
    print(f"æ€»å…±å¤„ç†: {len(df)} æ¡è®°å½•")
    print(f"å°çº¢ä¹¦: {len(df[df['å¹³å°'] == 'å°çº¢ä¹¦'])} æ¡")
    print(f"æŠ–éŸ³: {len(df[df['å¹³å°'] == 'æŠ–éŸ³'])} æ¡")
    print(f"ç»“æœå·²ä¿å­˜åˆ°: {filepath}")
    
    return filepath

def get_douyin_id(url: str) -> str:
    if "v.douyin.com" in url:
        response = requests.get(url, allow_redirects=False, timeout=10)
        if response.status_code in [301, 302, 307]:
            url = response.headers.get('Location', url)
            print(f"dy Redirected to: {url}")
    match = re.search(r'note/(\d+)', url)
    if match:
        return match.group(1)
    else:
        raise ValueError(f"Invalid Douyin URL: {url}. Unable to extract ID.")


async def crawl_data(input_text, save_path):
    """çˆ¬å–æ•°æ®çš„æ ¸å¿ƒé€»è¾‘"""
    # Delete all JSON files in data directories
    douyin_json_path = "./data/douyin/json/"
    xhs_json_path = "./data/xhs/json/"
    
    for path in [douyin_json_path, xhs_json_path]:
        if os.path.exists(path):
            json_files = glob.glob(os.path.join(path, "*.json"))
            for json_file in json_files:
                try:
                    os.remove(json_file)
                    logger.info(f"Deleted: {json_file}")
                except Exception as e:
                    logger.error(f"Failed to delete {json_file}: {e}")
    
    config.CRAWLER_TYPE = "detail"

    links = extract_links_from_text(input_text)

    xhslinks = [link for link in links if link['platform'] == 'å°çº¢ä¹¦']
    dylinks = [link for link in links if link['platform'] == 'æŠ–éŸ³']
    config.XHS_SPECIFIED_NOTE_URL_LIST = [link['url'] for link in xhslinks]
    config.DY_SPECIFIED_ID_LIST = [get_douyin_id(link['url']) for link in dylinks]

    crawler = CrawlerFactory.create_crawler(platform="xhs")
    await crawler.start()
    
    crawler = CrawlerFactory.create_crawler(platform="dy")
    await crawler.start()
    
    # çˆ¬å–å®Œæˆåï¼Œç”ŸæˆExcelæ–‡ä»¶
    return save_to_excel(save_path)


class CrawlerGUI:
    def __init__(self, root):
        self.root = root
        self.root.title("ç¤¾äº¤åª’ä½“æ•°æ®çˆ¬è™«å·¥å…·")
        self.root.geometry("800x600")
        
        # åˆ›å»ºä¸»æ¡†æ¶
        main_frame = ttk.Frame(root, padding="10")
        main_frame.grid(row=0, column=0, sticky=(tk.W, tk.E, tk.N, tk.S))
        
        # é…ç½®ç½‘æ ¼æƒé‡
        root.columnconfigure(0, weight=1)
        root.rowconfigure(0, weight=1)
        main_frame.columnconfigure(0, weight=1)
        main_frame.rowconfigure(1, weight=1)
        
        # æ ‡é¢˜
        title_label = ttk.Label(main_frame, text="ç¤¾äº¤åª’ä½“é“¾æ¥æ•°æ®çˆ¬å–å·¥å…·", font=("Arial", 16, "bold"))
        title_label.grid(row=0, column=0, pady=(0, 10), sticky=tk.W)
        
        # è¾“å…¥æ¡†æ ‡ç­¾
        input_label = ttk.Label(main_frame, text="è¯·è¾“å…¥åŒ…å«å°çº¢ä¹¦æˆ–æŠ–éŸ³é“¾æ¥çš„æ–‡æœ¬ï¼Œæ¯è¡Œä¸€æ¡ï¼š")
        input_label.grid(row=1, column=0, sticky=tk.W, pady=(0, 5))
        
        # è¾“å…¥æ¡†
        self.text_input = scrolledtext.ScrolledText(main_frame, height=20, width=80)
        self.text_input.grid(row=2, column=0, sticky=(tk.W, tk.E, tk.N, tk.S), pady=(0, 10))
        
        # è®¾ç½®é»˜è®¤æ–‡æœ¬
        default_text = """20 å·§å…‹åŠ›é…±é…±.å‘å¸ƒäº†ä¸€ç¯‡å°çº¢ä¹¦ç¬”è®°ï¼Œå¿«æ¥çœ‹å§ï¼ ğŸ˜† rDVoUzA9mQWI5MJ ğŸ˜† http://xhslink.com/a/9xEPz6muLDofbï¼Œå¤åˆ¶æœ¬æ¡ä¿¡æ¯ï¼Œæ‰“å¼€ã€å°çº¢ä¹¦ã€‘AppæŸ¥çœ‹ç²¾å½©å†…å®¹ï¼
52 æ‹¾è´°å‘å¸ƒäº†ä¸€ç¯‡å°çº¢ä¹¦ç¬”è®°ï¼Œå¿«æ¥çœ‹å§ï¼ ğŸ˜† ZVxybzkZNa7lWEU ğŸ˜† http://xhslink.com/a/LBoH4igKHbofbï¼Œå¤åˆ¶æœ¬æ¡ä¿¡æ¯ï¼Œæ‰“å¼€ã€å°çº¢ä¹¦ã€‘AppæŸ¥çœ‹ç²¾å½©å†…å®¹ï¼
2.84 å¤åˆ¶æ‰“å¼€æŠ–éŸ³ï¼Œçœ‹çœ‹ã€é›ªæ—¶çš„å›¾æ–‡ä½œå“ã€‘å‰ä¼Šæ¼”å”±ä¼šã€‚# chiikawaå‰ä¼Š # chii... https://v.douyin.com/qAvG3Q0bJbc/ m@Q.XZ 07/15 icn:/
4.69 å¤åˆ¶æ‰“å¼€æŠ–éŸ³ï¼Œçœ‹çœ‹ã€é£è‹’çš„å›¾æ–‡ä½œå“ã€‘æ¸©è¿ªç”Ÿæ—¥å¿«ä¹ï¼ä¸ç®¡æ˜¯ä»€ä¹ˆæ ·çš„æ¸©è¿ªéƒ½æ˜¯ä¸€æ ·çš„å¯çˆ±å‘¢ ... https://v.douyin.com/ckBTSnFL_1Y/ eOX:/ G@i.ca 05/05"""
        self.text_input.insert("1.0", default_text)
        
        # æŒ‰é’®æ¡†æ¶
        button_frame = ttk.Frame(main_frame)
        button_frame.grid(row=3, column=0, pady=10)
        
        # è·å–æ•°æ®æŒ‰é’®
        self.crawl_button = ttk.Button(button_frame, text="è·å–æ•°æ®", command=self.start_crawling)
        self.crawl_button.pack(side=tk.LEFT, padx=(0, 10))
        
        # æ¸…é™¤ç™»å½•ä¿¡æ¯æŒ‰é’®
        self.clear_login_button = ttk.Button(button_frame, text="æ¸…é™¤ç™»å½•ä¿¡æ¯", command=self.clear_login_data)
        self.clear_login_button.pack(side=tk.LEFT, padx=(0, 10))
        
        # è¾“å…¥æ¡†æ ‡ç­¾
        input_label = ttk.Label(main_frame, text="ä½¿ç”¨æ—¶ä¼šå¼¹å‡ºå°çº¢ä¹¦å’ŒæŠ–éŸ³ç™»å½•é¡µé¢ï¼Œéœ€è¦ç™»é™†åæ‰èƒ½è·å–æ•°æ®ã€‚ç™»å½•å°çº¢ä¹¦ä¼šä¼ªè£…æˆ Mac OS X è®¾å¤‡ï¼Œè¯·æ”¾å¿ƒä½¿ç”¨")
        input_label.grid(row=4, column=0, sticky=tk.W, pady=(0, 5))
        
        
        # çŠ¶æ€æ ‡ç­¾
        self.status_label = ttk.Label(main_frame, text="å‡†å¤‡å°±ç»ª", foreground="green")
        self.status_label.grid(row=5, column=0, sticky=tk.W, pady=(10, 0))
    
    def clear_login_data(self):
        """æ¸…é™¤ç™»å½•ä¿¡æ¯"""
        try:
            browser_data_path = "./browser_data"
            if os.path.exists(browser_data_path):
                # åˆ é™¤browser_dataç›®å½•ä¸‹çš„æ‰€æœ‰å†…å®¹
                for item in os.listdir(browser_data_path):
                    item_path = os.path.join(browser_data_path, item)
                    if os.path.isdir(item_path):
                        shutil.rmtree(item_path)
                    else:
                        os.remove(item_path)
                
                self.status_label.config(text="ç™»å½•ä¿¡æ¯å·²æ¸…é™¤", foreground="green")
                messagebox.showinfo("æˆåŠŸ", "ç™»å½•ä¿¡æ¯å·²æ¸…é™¤ï¼ä¸‹æ¬¡ä½¿ç”¨æ—¶éœ€è¦é‡æ–°ç™»å½•ã€‚")
                logger.info("Browser data cleared successfully")
            else:
                messagebox.showinfo("æç¤º", "æ²¡æœ‰æ‰¾åˆ°ç™»å½•ä¿¡æ¯æ–‡ä»¶")
                
        except Exception as e:
            error_msg = f"æ¸…é™¤ç™»å½•ä¿¡æ¯å¤±è´¥: {str(e)}"
            self.status_label.config(text="æ¸…é™¤å¤±è´¥", foreground="red")
            messagebox.showerror("é”™è¯¯", error_msg)
            logger.error(error_msg)
        
    def start_crawling(self):
        """å¼€å§‹çˆ¬å–æ•°æ®"""
        input_text = self.text_input.get("1.0", tk.END).strip()
        
        if not input_text:
            messagebox.showerror("é”™è¯¯", "è¯·è¾“å…¥è¦çˆ¬å–çš„æ–‡æœ¬å†…å®¹ï¼")
            return
        
        # ç”Ÿæˆé»˜è®¤æ–‡ä»¶å
        current_time = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        default_filename = f"ç‚¹è¯„èµæ•°æ®-{current_time}.xlsx"
        
        # è®©ç”¨æˆ·é€‰æ‹©ä¿å­˜è·¯å¾„
        save_path = filedialog.asksaveasfilename(
            title="é€‰æ‹©ä¿å­˜ä½ç½®",
            defaultextension=".xlsx",
            filetypes=[("Excel files", "*.xlsx"), ("All files", "*.*")],
            initialfile=default_filename
        )
        
        if not save_path:  # ç”¨æˆ·å–æ¶ˆäº†ä¿å­˜å¯¹è¯æ¡†
            return
        
        # ç¦ç”¨æŒ‰é’®
        self.crawl_button.config(state="disabled")
        self.clear_login_button.config(state="disabled")
        self.status_label.config(text="æ­£åœ¨çˆ¬å–æ•°æ®ï¼Œè¯·ç¨å€™...", foreground="blue")
        
        # åœ¨æ–°çº¿ç¨‹ä¸­è¿è¡Œçˆ¬è™«
        def run_crawler():
            try:
                loop = asyncio.new_event_loop()
                asyncio.set_event_loop(loop)
                result = loop.run_until_complete(crawl_data(input_text, save_path))
                loop.close()
                
                # åœ¨ä¸»çº¿ç¨‹ä¸­æ›´æ–°UI
                self.root.after(0, self.crawling_finished, result)
                
            except Exception as e:
                self.root.after(0, self.crawling_error, str(e))
        
        threading.Thread(target=run_crawler, daemon=True).start()
    
    def crawling_finished(self, result_file):
        """çˆ¬å–å®Œæˆåçš„å¤„ç†"""
        self.crawl_button.config(state="normal")
        self.clear_login_button.config(state="normal")
        self.status_label.config(text=f"çˆ¬å–å®Œæˆï¼ç»“æœå·²ä¿å­˜åˆ°: {result_file}", foreground="green")
        messagebox.showinfo("å®Œæˆ", f"æ•°æ®çˆ¬å–å®Œæˆï¼\nç»“æœå·²ä¿å­˜åˆ°: {result_file}")
    
    def crawling_error(self, error_msg):
        """çˆ¬å–å‡ºé”™æ—¶çš„å¤„ç†"""
        self.crawl_button.config(state="normal")
        self.clear_login_button.config(state="normal")
        self.status_label.config(text="çˆ¬å–å¤±è´¥", foreground="red")
        messagebox.showerror("é”™è¯¯", f"çˆ¬å–è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯ï¼š\n{error_msg}")


def start_gui():
    """å¯åŠ¨GUIåº”ç”¨"""
    root = tk.Tk()
    app = CrawlerGUI(root)
    root.mainloop()


if __name__ == '__main__':
    try:
        # å¯åŠ¨GUIè€Œä¸æ˜¯ç›´æ¥è¿è¡Œçˆ¬è™«
        start_gui()
    except KeyboardInterrupt:
        sys.exit()
